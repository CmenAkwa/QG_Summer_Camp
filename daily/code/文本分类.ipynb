{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "def tokenize(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # 去除HTML标签\n",
    "    text = re.sub(r'[^a-zA-Z0-9]', ' ', text)  # 只保留字母和数字\n",
    "    text = text.lower().split()\n",
    "    return text\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, train=True, max_len=500):\n",
    "        self.train_data_path = r\"./aclImdb/train\"\n",
    "        self.test_data_path = r\"./aclImdb/test\"\n",
    "        data_path = self.train_data_path if train else self.test_data_path\n",
    "        # 所有文件名放入列表\n",
    "        temp_data_path = [os.path.join(data_path, \"pos\"), os.path.join(data_path, \"neg\")]\n",
    "        self.total_file_path = []  # 所有评论文件路径\n",
    "        self.labels = []  # 标签列表\n",
    "        self.max_len = max_len  # 最大序列长度\n",
    "        for path in temp_data_path:\n",
    "            file_name_list = os.listdir(path)\n",
    "            file_path_list = [os.path.join(path, i) for i in file_name_list if i.endswith(\".txt\")]\n",
    "            self.total_file_path.extend(file_path_list)\n",
    "            self.labels.extend([1 if \"pos\" in path else 0] * len(file_name_list))\n",
    "        \n",
    "        # 构建词汇表\n",
    "        self.vocab = self.build_vocab()\n",
    "        \n",
    "    def build_vocab(self, max_vocab_size=20000):\n",
    "        counter = Counter()\n",
    "        for file_path in self.total_file_path:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                tokens = tokenize(text)\n",
    "                counter.update(tokens)\n",
    "        vocab = {word: idx + 2 for idx, (word, _) in enumerate(counter.most_common(max_vocab_size))}\n",
    "        vocab['<PAD>'] = 0\n",
    "        vocab['<UNK>'] = 1\n",
    "        return vocab\n",
    "    \n",
    "    def text_to_sequence(self, text):\n",
    "        tokens = tokenize(text)\n",
    "        sequence = [self.vocab.get(word, self.vocab['<UNK>']) for word in tokens]\n",
    "        if len(sequence) < self.max_len:\n",
    "            sequence += [self.vocab['<PAD>']] * (self.max_len - len(sequence))\n",
    "        else:\n",
    "            sequence = sequence[:self.max_len]\n",
    "        return sequence\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.total_file_path[index]\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        sequence = self.text_to_sequence(text)\n",
    "        label = self.labels[index]\n",
    "        return torch.tensor(sequence, dtype=torch.long), torch.tensor(label, dtype=torch.float)  # 修改标签为浮点数类型\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.total_file_path)\n",
    "\n",
    "def get_dataloader(train=True, batch_size=32):\n",
    "    dataset = MyDataset(train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # 添加一个新的全连接层\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()  # 使用ReLU激活函数\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "        # 通过第一个全连接层和激活函数\n",
    "        hidden = self.fc1(hidden)\n",
    "        hidden = self.relu(hidden)\n",
    "        \n",
    "        # 通过第二个全连接层\n",
    "        output = self.fc2(self.dropout(hidden))\n",
    "        \n",
    "        return output\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, labels = batch\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        print(epoch + 1, loss.item())\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 超参数\n",
    "INPUT_DIM = len(MyDataset().vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 5\n",
    "\n",
    "# 模型实例化\n",
    "model = LSTMModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT).to(device)\n",
    "\n",
    "# 优化器和损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=2e-5)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = get_dataloader(train=True, batch_size=BATCH_SIZE)\n",
    "test_loader = get_dataloader(train=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    input()\n",
    "    print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ed0a622c98d3b42d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
